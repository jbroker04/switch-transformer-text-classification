{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1cLhnHqVGOEQevPb1dbeA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### IMPORTS"
      ],
      "metadata": {
        "id": "CqvbQ259cW7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "74aHtixyHOip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f596eeb9-3ec2-494c-fd1c-e4a42013e627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "ElZoAQRHLIGQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "y4Qez2wk1wj2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOWNLOADING AND PREPARING DATASET"
      ],
      "metadata": {
        "id": "sHybJYgYcaBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "num_tokens_per_example = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=num_tokens_per_example)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=num_tokens_per_example)"
      ],
      "metadata": {
        "id": "LcyhZGU9LUTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2286a1ca-44d5-4c8a-f2ca-249dc9155420"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEFINING HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "jkAoQhRncgK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 16  # Embedding size for each token.\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 16  # Hidden layer size in feedforward network.\n",
        "num_experts = 5  # Number of experts used in the Switch Transformer.\n",
        "batch_size = 50  # Batch size.\n",
        "learning_rate = 0.001  # Learning rate.\n",
        "dropout_rate = 0.5  # Increase dropout rate\n",
        "num_epochs = 3  # Number of epochs.\n",
        "num_tokens_per_batch = (\n",
        "    batch_size * num_tokens_per_example\n",
        ")  # Total number of tokens per batch.\n",
        "print(f\"Number of tokens per batch: {num_tokens_per_batch}\")"
      ],
      "metadata": {
        "id": "X-7hMGddcghH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102aaf60-c721-4a8b-8601-4eb07db4490b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens per batch: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING TOKEN & POSITIONING EMBEDDING LAYER"
      ],
      "metadata": {
        "id": "nEksI-J6fC4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "      maxlen = tf.shape(x)[-1]\n",
        "      positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "      positions = self.pos_emb(positions)\n",
        "      x = self.token_emb(x)\n",
        "      return x + positions"
      ],
      "metadata": {
        "id": "5HWOJwpffKqX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING FEEDFORWARD NETWORK"
      ],
      "metadata": {
        "id": "zZRxTs6OgtVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feedforward_network(ff_dim, embed_dim, name=None):\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "            layers.Dense(embed_dim, kernel_regularizer=regularizers.l2(0.01))\n",
        "        ],\n",
        "        name=name\n",
        "    )"
      ],
      "metadata": {
        "id": "OTCVqlfRgtrg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING LOAD-BALANCED LOSS"
      ],
      "metadata": {
        "id": "PQ_F_iE0jSEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_balanced_loss(router_probs, expert_mask):\n",
        "    num_experts = tf.shape(expert_mask)[-1]\n",
        "    density = tf.reduce_mean(expert_mask, axis=0)\n",
        "    density_proxy = tf.reduce_mean(router_probs, axis=0)\n",
        "    loss = tf.reduce_mean(density_proxy * density) * tf.cast((num_experts**2), tf.float32)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "4Gk4aEjajScg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING ROUTER AS LAYER"
      ],
      "metadata": {
        "id": "KT8bvryWkohK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Router(layers.Layer):\n",
        "    def __init__(self, num_experts, expert_capacity):\n",
        "        self.num_experts = num_experts\n",
        "        self.route = layers.Dense(units=num_experts)\n",
        "        self.expert_capacity = expert_capacity\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        router_logits = self.route(inputs)\n",
        "\n",
        "        if training:\n",
        "            router_logits += tf.random.uniform(shape=tf.shape(router_logits), minval=0.9, maxval=1.1)\n",
        "        router_probs = tf.nn.softmax(router_logits, axis=-1)\n",
        "        expert_gate, expert_index = tf.math.top_k(router_probs, k=1)\n",
        "        expert_mask = tf.one_hot(expert_index, self.num_experts)\n",
        "        aux_loss = load_balanced_loss(router_probs, expert_mask)\n",
        "        self.add_loss(aux_loss)\n",
        "\n",
        "        position_in_expert = tf.cast(tf.cumsum(expert_mask, axis=0) * expert_mask, tf.int32)\n",
        "        expert_mask *= tf.cast(tf.less(position_in_expert, self.expert_capacity), tf.float32)\n",
        "        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n",
        "        expert_gate *= expert_mask_flat\n",
        "\n",
        "        combined_tensor = tf.expand_dims(\n",
        "            expert_gate * expert_mask_flat * tf.squeeze(tf.one_hot(expert_index, self.num_experts), 1),\n",
        "            -1,\n",
        "        ) * tf.squeeze(tf.one_hot(position_in_expert, self.expert_capacity), 1)\n",
        "        dispatch_tensor = tf.cast(combined_tensor, tf.float32)\n",
        "\n",
        "        return dispatch_tensor, combined_tensor"
      ],
      "metadata": {
        "id": "VU2HY37Gko7x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING SWITCH LAYER"
      ],
      "metadata": {
        "id": "N9Rq1FdjlJBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Switch(layers.Layer):\n",
        "    def __init__(\n",
        "        self, num_experts, embed_dim, ff_dim, num_tokens_per_batch, capacity_factor=1\n",
        "    ):\n",
        "        self.num_experts = num_experts\n",
        "        self.embed_dim = embed_dim\n",
        "        self.experts = [\n",
        "            create_feedforward_network(ff_dim, embed_dim) for _ in range(num_experts)\n",
        "        ]\n",
        "\n",
        "        self.expert_capacity = num_tokens_per_batch // self.num_experts\n",
        "        self.router = Router(self.num_experts, self.expert_capacity)\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        num_tokens_per_example = tf.shape(inputs)[1]\n",
        "\n",
        "        inputs = tf.reshape(inputs, [num_tokens_per_batch, self.embed_dim])\n",
        "        dispatch_tensor, combine_tensor = self.router(inputs)\n",
        "        expert_inputs = tf.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n",
        "        expert_inputs = tf.reshape(\n",
        "            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n",
        "        )\n",
        "        expert_input_list = tf.unstack(expert_inputs, axis=0)\n",
        "        expert_output_list = [\n",
        "            self.experts[idx](expert_input)\n",
        "            for idx, expert_input in enumerate(expert_input_list)\n",
        "        ]\n",
        "        expert_outputs = tf.stack(expert_output_list, axis=1)\n",
        "        expert_outputs_combined = tf.einsum(\n",
        "            \"abc,xba->xc\", expert_outputs, combine_tensor\n",
        "        )\n",
        "        outputs = tf.reshape(\n",
        "            expert_outputs_combined,\n",
        "            [batch_size, num_tokens_per_example, self.embed_dim],\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "M56DBnoFlJY6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENT TRANSFORMER BLOCK LAYER"
      ],
      "metadata": {
        "id": "jFZJWBZ97U0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ffn, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = ffn\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "C_amLcUp7Vdt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING CLASSIFIER"
      ],
      "metadata": {
        "id": "LLakldZC7UvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_classifier():\n",
        "    switch = Switch(num_experts, embed_dim, ff_dim, num_tokens_per_batch)\n",
        "    transformer_block = TransformerBlock(embed_dim // num_heads, num_heads, switch)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_tokens_per_example,))\n",
        "    embedding_layer = TokenAndPositionEmbedding(\n",
        "        num_tokens_per_example, vocab_size, embed_dim\n",
        "    )\n",
        "    x = embedding_layer(inputs)\n",
        "    x = transformer_block(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    classifier = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "4qkFtqdkAcw7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "0v7QgjGFClfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(classifier):\n",
        "    classifier.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    history = classifier.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "    return history\n",
        "\n",
        "classifier = create_classifier()\n",
        "run_experiment(classifier)"
      ],
      "metadata": {
        "id": "K480lDQYCl4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8720c919-302d-499a-9ebd-f5975cb630c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "500/500 [==============================] - 88s 164ms/step - loss: 1.9611 - accuracy: 0.7098 - val_loss: 1.3542 - val_accuracy: 0.8518\n",
            "Epoch 2/3\n",
            "500/500 [==============================] - 62s 125ms/step - loss: 1.2887 - accuracy: 0.8979 - val_loss: 1.3075 - val_accuracy: 0.8720\n",
            "Epoch 3/3\n",
            "500/500 [==============================] - 58s 117ms/step - loss: 1.2054 - accuracy: 0.9331 - val_loss: 1.3503 - val_accuracy: 0.8685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x794095615e40>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}